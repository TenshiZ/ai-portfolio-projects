# ğŸ§© Malicious Code Injection in Machine Learning Models (TFG)

This project is my **Final Degree Project (TFG)**, focused on exploring and mitigating **security vulnerabilities in machine learning model serialization**.  
It investigates how adversarial payloads can be embedded into ML artifacts and proposes **detection and defense mechanisms** to secure the AI supply chain.

---

## ğŸ§  Overview
The research explores multiple attack surfaces in the ML lifecycle â€” from unsafe deserialization to poisoned models â€” using **controlled and non-destructive experiments** with both **PyTorch** and **TensorFlow**.

The study covers:
- Insecure serialization methods (`__reduce__`, custom deserialization hooks, pickle/HDF5/SavedModel)
- Malicious payload injection and tensor manipulation
- Model-poisoning through altered preprocessing
- Safe detection and mitigation scripts
- Recommendations for secure model loading and artifact validation

---

## ğŸ“‚ Repository Structure

ml-model-injection/
â”‚
â”œâ”€â”€ codigo/
â”‚ â”œâ”€â”€ ataque_metodos/ # Tests comparing safe vs. unsafe model methods
â”‚ â”‚ â”œâ”€â”€ safe_model/ # Secure serialization example
â”‚ â”‚ â””â”€â”€ unsafe_model/ # Vulnerable serialization example
â”‚ â”‚ â”œâ”€â”€ detection.py # Validation and comparison script
â”‚ â”‚ â”œâ”€â”€ inject_tensor.py # Controlled tensor-level injection demo
â”‚ â”‚ â””â”€â”€ tensorflow_codeinject.py# Safe TensorFlow deserialization test
â”‚ â”‚
â”‚ â”œâ”€â”€ ataque_payload/ # Experiments with adversarial payload injection
â”‚ â”‚ â”œâ”€â”€ add_payload.py # Controlled embedding of non-malicious payloads
â”‚ â”‚ â”œâ”€â”€ inject_attack.py # Simulated injection process (safe)
â”‚ â”‚ â”œâ”€â”€ detection.py # Payload detection script
â”‚ â”‚ â”œâ”€â”€ cats_and_dogs_mobilenet.py # Model used for testing (transfer learning)
â”‚ â”‚ â”œâ”€â”€ ModeloEject.pt # Example model for controlled test
â”‚ â”‚ â””â”€â”€ gato.jpg # Sample input image
â”‚ â”‚
â”‚ â”œâ”€â”€ ataque_reduce/ # Research on PyTorch reduce and deserialization
â”‚ â”‚ â”œâ”€â”€ inject_reduce.py # Controlled test for reduce vulnerability
â”‚ â”‚ â”œâ”€â”€ detection.py # Detection script for reduce-based payloads
â”‚ â”‚ â”œâ”€â”€ ModeloDetect.pt # Sample model
â”‚ â”‚ â””â”€â”€ gato.jpg
â”‚ â”‚
â”‚ â””â”€â”€ cats_and_dogs_filtered/ # Dataset used in all experiments
â”‚ â”œâ”€â”€ train/
â”‚ â””â”€â”€ validation/
â”‚
â”œâ”€â”€ material_adicional/ # Complementary resources
â”‚ â”œâ”€â”€ videos.zip # Recorded demonstrations (safe examples)
â”‚ â””â”€â”€ material_adicional.zip # Extra references and figures
â”‚
â”œâ”€â”€ TFG_Ãngel_Antequera_GÃ³mez.pdf # Full written report (Final Degree Project)
â”‚
â””â”€â”€ README.md # Documentation (this file)

---

## ğŸ§° Highlights
- Analysis of **serialization vulnerabilities** in PyTorch and TensorFlow  
- Controlled **payload injection** and **tensor manipulation** demonstrations   
- **Dataset** (`cats_and_dogs_filtered`) for reproducible experiments  
- Complete **academic report** and visual demonstrations in `/material_adicional/`

---

## âš™ï¸ Tech Stack
Python Â· PyTorch Â· TensorFlow Â· Keras Â· NumPy Â· Pandas Â· Matplotlib 

---

## ğŸ“„ Included Files
- **`TFG_Ãngel_Antequera_GÃ³mez.pdf`** â€” Full academic report (methodology, results, and references).  
- **`codigo/`** â€” All controlled experimental code, detection tools, and dataset.  
- **`material_adicional/`** â€” Videos and supporting materials for demonstrations.  

---

## ğŸ’¬ Notes
- All scripts and demonstrations are **non-destructive** and serve educational and research purposes only.  
- No harmful or malicious payloads are distributed.  
- All tests were executed in isolated environments with academic supervision.

---

## ğŸ’¡ Purpose
This project aims to raise awareness about **security risks in AI workflows**, highlighting how model artifacts can be potential attack vectors â€” and how to detect and mitigate them responsibly.  
It reflects my broader goal of building **secure, trustworthy, and explainable AI systems**.

---

## âš ï¸ Disclaimer
All experiments and code included in this repository are intended strictly for **research and educational purposes**.  
No harmful or malicious use is intended.  
If you reproduce or extend this work, please follow applicable laws, academic ethics, and responsible disclosure practices.